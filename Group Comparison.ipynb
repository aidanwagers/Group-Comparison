{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c85caf",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f9dbac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aidan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "\n",
    "from nltk.book import *\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint # get some prettier printing of objects\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "import sqlite3\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f4f818",
   "metadata": {},
   "source": [
    "# SQLite Connection and Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c03a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"ConventionSpeeches.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697de370",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                                SELECT text, party\n",
    "                                FROM conventions\n",
    "                                WHERE speaker != \"Unknown\"\n",
    "                            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aad2f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = defaultdict(str)\n",
    "\n",
    "for row in query_results :\n",
    "    text, party = row\n",
    "\n",
    "    # Clean text\n",
    "    text = \"\".join([ch for ch in text if ch not in punctuation])    \n",
    "    text = [w.lower() for w in text.split() if w.isalpha()]\n",
    "    \n",
    "    convention_data[party] += \" \".join(text) + \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8341e9",
   "metadata": {},
   "source": [
    "# Function Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "490c77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_function(corpus_1,corpus_2,num_words,ratio_cutoff):\n",
    "    #input_list = (corpus_1,corpus_2)\n",
    "    combined_dict1 = {}\n",
    "    combined_dict2 = {}\n",
    "    oneV2Dict = {}\n",
    "    twoV1Dict = {}\n",
    "    outputDict = {\"one\":combined_dict1,\"two\":combined_dict2,\"one_vs_two\": oneV2Dict,\"two_vs_one\":twoV1Dict}\n",
    "    \n",
    "    c = Counter()\n",
    "    \n",
    "    total_tokens = 1\n",
    "    unique_tokens = 0\n",
    "    avg_token_len = 0.0\n",
    "    lex_diversity = 0.0\n",
    "    top_10 = Counter()\n",
    "    \n",
    "    #Summary Stats for 1\n",
    "    c1_clean = [word for word in corpus_1]\n",
    "    c1_clean = [word.lower() for word in c1_clean if word.isalpha() and word.lower() not in sw]\n",
    "    \n",
    "    \n",
    "    c.update(c1_clean)\n",
    "        \n",
    "     # Calculate your statistics here\n",
    "    total_tokens = len(c1_clean)\n",
    "    unique_tokens = len(set(c1_clean))\n",
    "    lex_diversity = (unique_tokens/total_tokens)\n",
    "    \n",
    "    \n",
    "    token_len = [len(word) for word in c1_clean]\n",
    "    \n",
    "    avg_token_len = np.mean(token_len)\n",
    "    top_10 = c.most_common(10)\n",
    "    \n",
    "    combined_dict1 = {'tokens':total_tokens,\n",
    "                'unique_tokens':unique_tokens,\n",
    "                'avg_token_length':avg_token_len,\n",
    "                'lexical_diversity':lex_diversity,\n",
    "                'top_words':top_10}\n",
    "    \n",
    "    #Summary Stats for 2\n",
    "    c2_clean = [word for word in corpus_2]\n",
    "    c2_clean = [word.lower() for word in c2_clean if word.isalpha() and word.lower() not in sw]\n",
    "    \n",
    "    \n",
    "    c.update(c2_clean)\n",
    "        \n",
    "     # Calculate your statistics here\n",
    "    total_tokens = len(c2_clean)\n",
    "    unique_tokens = len(set(c2_clean))\n",
    "    lex_diversity = (unique_tokens/total_tokens)\n",
    "    \n",
    "    \n",
    "    token_len = [len(word) for word in c2_clean]\n",
    "    \n",
    "    avg_token_len = np.mean(token_len)\n",
    "    top_10 = c.most_common(10)\n",
    "    \n",
    "        # Now we'll fill out the dictionary. \n",
    "    combined_dict2 = {'tokens':total_tokens,\n",
    "                'unique_tokens':unique_tokens,\n",
    "                'avg_token_length':avg_token_len,\n",
    "                'lexical_diversity':lex_diversity,\n",
    "                'top_words':top_10}\n",
    "    \n",
    "    freq1 = nltk.FreqDist(c1_clean)\n",
    "    freq2 = nltk.FreqDist(c2_clean)\n",
    "    \n",
    "    corpus_one_concentration = defaultdict(float)\n",
    "    corpus_two_concentration = defaultdict(float)\n",
    "    \n",
    "    for word, freq in freq1.items():\n",
    "        corpus_one_concentration[word] = freq/len(c1_clean)\n",
    "    \n",
    "    for word, freq in freq2.items():\n",
    "        corpus_two_concentration[word] = freq/len(c2_clean)\n",
    "        \n",
    "    \n",
    "    \n",
    "    oneV2Dict = {}\n",
    "    twoV1Dict = {}\n",
    "    \n",
    "    for word in corpus_one_concentration:\n",
    "        if ratio_cutoff > freq1[word] or ratio_cutoff > freq2[word]:\n",
    "            continue\n",
    "        ratio1 = corpus_one_concentration[word]/corpus_two_concentration[word] \n",
    "        \n",
    "        oneV2Dict[word] = ratio1\n",
    "    \n",
    "    for word in corpus_two_concentration:\n",
    "        if ratio_cutoff > freq1[word] or ratio_cutoff > freq2[word]:\n",
    "            continue\n",
    "        ratio2 = corpus_two_concentration[word]/corpus_one_concentration[word] \n",
    "        \n",
    "        twoV1Dict[word] = ratio2\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    #need to figure out how to limit to num_words for one_v2 and two_vone\n",
    "    outputDict = {\"one\":combined_dict1,\"two\":combined_dict2,\"one_vs_two\": sorted(oneV2Dict.items(), key = lambda item:item[1], reverse = True)[:num_words],\"two_vs_one\":sorted(twoV1Dict.items(), key = lambda item:item[1], reverse = True)[:num_words]}\n",
    "    print(outputDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ef6037cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_split = convention_data['Democratic'].split()\n",
    "r_split = convention_data['Republican'].split()\n",
    "big = open(\"big.txt\",'r').read()\n",
    "big = big.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc771eba",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "88837d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'tokens': 154088, 'unique_tokens': 13158, 'avg_token_length': 6.132599553501895, 'lexical_diversity': 0.0853927625772286, 'top_words': [('president', 1699), ('america', 1504), ('us', 1195), ('people', 1009), ('american', 986), ('country', 985), ('trump', 878), ('know', 829), ('thank', 796), ('years', 787)]}, 'two': {'tokens': 238698, 'unique_tokens': 15011, 'avg_token_length': 6.0489656385893475, 'lexical_diversity': 0.0628869952827422, 'top_words': [('president', 4133), ('america', 3856), ('us', 3283), ('people', 2615), ('country', 2306), ('american', 2219), ('know', 2082), ('john', 2040), ('one', 2018), ('years', 1860)]}, 'one_vs_two': [('radical', 15.835262966616478), ('media', 15.684655846010072), ('sarah', 11.360079954311823), ('gulf', 11.360079954311823), ('islamic', 9.552794507034942), ('usa', 8.606121177508955), ('prayers', 7.74550905975806), ('ann', 6.712774518456985), ('deals', 6.368529671356627), ('regulations', 6.337234685256596)], 'two_vs_one': [('edwards', 38.86122883308616), ('auto', 10.974101165489447), ('climate', 8.822316623236615), ('kerry', 7.352805228527363), ('kennedy', 7.020197069099867), ('kamala', 6.916450314384105), ('bernie', 6.631408725670093), ('native', 6.197139481688159), ('middleclass', 5.91740749119529), ('barack', 5.900193214857267)]}\n"
     ]
    }
   ],
   "source": [
    "the_function(r_split, d_split,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "766ad8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one': {'tokens': 154088, 'unique_tokens': 13158, 'avg_token_length': 6.132599553501895, 'lexical_diversity': 0.0853927625772286, 'top_words': [('president', 1699), ('america', 1504), ('us', 1195), ('people', 1009), ('american', 986), ('country', 985), ('trump', 878), ('know', 829), ('thank', 796), ('years', 787)]}, 'two': {'tokens': 410950, 'unique_tokens': 25173, 'avg_token_length': 6.361155858376931, 'lexical_diversity': 0.06125562720525611, 'top_words': [('one', 3652), ('said', 3304), ('may', 2642), ('would', 2477), ('could', 1948), ('president', 1938), ('prince', 1713), ('us', 1650), ('american', 1648), ('america', 1642)]}, 'one_vs_two': [('applause', 388.3126525102539), ('tonight', 126.87216766671364), ('job', 93.72538512315784), ('bless', 82.94315585898967), ('cheers', 65.78556841977745), ('values', 54.67314132184206), ('lets', 45.71970006007699), ('communities', 43.11621713998927), ('economy', 37.651517696533986), ('democrat', 35.81376504706031)], 'two_vs_one': [('french', 62.692574765786595), ('shall', 53.39367611631586), ('andrew', 27.10928920793284), ('cases', 26.77182917629882), ('russian', 22.12237985156345), ('round', 21.70055481202093), ('usually', 17.55473901934542), ('seemed', 16.897998621081236), ('smile', 16.42305487285558), ('condition', 15.373179218883076)]}\n"
     ]
    }
   ],
   "source": [
    "the_function(r_split,big,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c21438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
